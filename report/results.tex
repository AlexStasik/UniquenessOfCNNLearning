\section{Results}

The cifar10 dataset was split into two halves that were found to give roughly equal classification accuracy by a single network trained to classify all 10 classes. However, when the data set was split up it turned out that one was somewhat easier than the other. We call these new datasets cifar5 dataset 1 and 2.

\subsection{Cifar5 transfer learning}

To begin with, a network with the 3 convolutional layers, consisting of 4, 8 and 16 filters respectively, and a single fully connected layer consisting of 128 neurons was trained multiple times on one of the two cifar10 halves. First, the networks were trained for 100 epochs and the best performing network state was saved. After the initial training, the best network state was loaded, the first layer was frozen and all subsequent layers were reinitialized, and for each of the two cifar5 datasets the network was trained for 100 more epochs. The continued training on the same network served as a reference point. This procedure was repeated 7 times for each dataset.

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=\textwidth]{\string~/Dropbox/uniqueness_cnn_figures/training_transfer_learning_cifar5.pdf}
\end{center}
\caption{Left side: initial training on both datasets. Right side: transfer learning with the first convolutional layer frozen.}
\label{fig:init_training_cifar5}
\end{figure}

\Fref{fig:init_training_cifar5} shows the training curves. The left side shows the initial training, from which the first convolutional layer is taken for the transfer learning shown on the right side. 
As can be seen, the network performs consistently better on dataset 2 compared to dataset 1. However, forcing the network to use the first convolutional layer trained on either of the dataset doesn't significantly hinder its performance on the opposite dataset. Regardess of whether the first layer was taken from the network trained on the same dataset or the other dataset, the perfomance was roughly equal.

To investigate whether or not this was due to the fact that the first filters were picking up the same features, the filters were flattened and rescaled to unity, and the dot product was computed between all filter pairs over all networks, for the same colour channels. The absolute values of the dot products are shown in \Fref{fig:dot_products_cifar5}. The values are generally low, and there doesn't seem to be any specific filters present in all networks. This implies that the network can find many roughly equally good solutions to its task. Since the transfer learning showed that the first filters learned on one dataset can be used to get roughly equally good performance on the other dataset, it also seems that the first-layer solutions found on one dataset will work equally well on the other dataset.

To see whether the learned filters for each network spanned the same vector space, we checked wheter each filter could be represented as a linear combination of all four filters in any of the other networks. However, the best fits were poor, the lowest residual norm was 0.43 and the average was 0.82. 


\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{\string~/Dropbox/uniqueness_cnn_figures/dset1_dot_products.pdf}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{\string~/Dropbox/uniqueness_cnn_figures/dset2_dot_products.pdf}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{\string~/Dropbox/uniqueness_cnn_figures/dset1_dset2_dot_products.pdf}
  \end{subfigure}
  \caption{Absolute value of dot products between all filters, which have been flattened and normalized, for each colour channel.}
  \label{fig:dot_products_cifar5}
\end{figure}

\newpage~\newpage

\paragraph{Freezing multiple layers in a smaller network}~\\

Since the previous network was able to achieve the same performance on each dataset regardless of whether the first layer was chosen from a network trained on the first or second dataset, a smaller network was chosen, still consisting of three convolutional layers and a single fully connected layer, but with 2, 4 and 8 filters in the convolutional layers, and 32 neurons in the fully connected layer.
\Fref{fig:freeze_multiple_conv_layers} shows the training curves for transfer learning with an increasing number of frozen convolutional layers. For each network, the convolutional layers are frozen and training is continued on both the opposite dataset and the same dataset for comparison.
Freezing a single layer, using the first convolutional layer from the network trained on dataset 2 performs equally well on dataset 1 when the subsequent layers are retrained.
In the opposite direction, using the first layer trained on dataset 1 causes a slight decrease in accuracy on dataset 2.
As more layers are frozen, the penalty of using convolutional layers trained on the opposite dataset increases.
The effect is most significant when using the layers trained on dataset 2 to transfer to dataset 1.
Using the convolutional layers trained on dataset 1 generally works well on dataset 2, although the difference also increases here when multiple layers are frozen.
This implies that the network trained on dataset 1 picks up features that are also present in dataset 2, while the the network trained on dataset 2 picks up features that are more specific to its dataset. This also agrees with the fact that the performance on dataset 2 is generally higher (see \Fref{fig:init_training_cifar5}).

There is a trend that the accuracy generally increases when multiple layers are frozen.
This is because fewer layers are reinitialized and a smaller portion of the network has to be trained from scratch.




\begin{figure}
  \begin{center}
    \includegraphics{\string~/Dropbox/uniqueness_cnn_figures/freeze_multiple_conv_layers.pdf}
  \end{center}
  \caption{Transfer learning with increasing number of frozen convolutional layers.}
  \label{fig:freeze_multiple_conv_layers}
\end{figure}



\subsection{Rotating filters in the first layer}

In addition to training with the first convolutional layer trained on the opposite dataset, we also tried freezing  both a gradient-like filter and a randomly initialized one to compare.
With a network consisting of one convolutional layer with two filters and a single fully connected layer, the filters were initialized with the same kernel, but where one of the filters was rotated.
No activation function was used for the convolutional layer.
\Fref{fig:rotated_gradient_filter} shows the rotated gradient filters.
The random filters are not shown, but they were rotated in the same manner.

The network was trained with the filters rotated by 0, 45, 90, 135, 180, 225, 270, 315 and 360 degrees.
\Fref{fig:rotated_filter_accuracies} shows the highest achieved accuracies for each rotation of the filters. For the gradient-like filter the accuracy is significantly lower when it is not rotated. Since both filters are identical, they give the exact same information. Since the gradient filter is symmetric, you'd expect the accuracy to be equally low at a 180$^\circ$ rotation since the exact same information is present in both filters at this angle. This is not the case, however, and the network is able to achieve better accuracy with the filter rotated 180$^\circ$. It's difficult to see why this should be the case, but it might mean that identical filters make the optimization more difficult and that the problem is not only a lack of information.

With the random filter, the accuracy does not change significantly with the rotations. The accuracy is generally a lot higher than for the gradient filter regardless of the rotations, implying that even a completely random filter extracts more information than the simple gradient filter.

\begin{figure}
  \begin{center}
    \includegraphics{\string~/Dropbox/uniqueness_cnn_figures/gradient_filter_rotation.pdf}
  \end{center}
  \caption{Rotated gradient filters. Each column shows both of the filters }
  \label{fig:rotated_gradient_filter}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics{\string~/Dropbox/uniqueness_cnn_figures/rotation_accuracies.pdf}
  \end{center}
  \vspace{5mm}
  \caption{Highest achieved accuracy for each rotation of the filters, both the gradient filter and the random one.}
  \label{fig:rotated_filter_accuracies}
\end{figure}


\subsection{Accuracy as a function of total parameters}

An attempt was made to see whether it is possible to predict the accuracy a network can achieves by increasing the total number of trainable parameters in the network. Two types of network architectures were chosen, one with an exponentially increasing number of filters in the convolutional layers, doubling in size for each layer, and one with a constant number of filters in the convolutional layers. A network with three convolutional layers were used, and a single dense layer. It was trained with a dense layer consisted of either 32, 64 or 128 neurons, and for the networks of constant-size convolutional layers the number of filters were 2, 4, 8, 16, 32, 64 and 160. For the exponentially increasing one, the first layer started out with 2, 4, 8, 16, 32 and 64 filters. For each combination of number of filters in the convolutional layers and dense neurons, the network was trained for 100 epochs, and the function $y = a - e^{-b/x}$ was fitted to the best achieved accuracies as a function of the total number of parameters in the network. \Fref{fig:accuracy_parameters_all} shows the data points and a least squares fit. The fit is poor and underestimates the accuracy with a high number of parameters.

\Fref{fig:accuracy_parameters_split} shows the data points grouped by the number of dense neurons and network type. In this plot, the curve is fitted to all but the highest parameter data point. The fit here is much better than in \Fref{fig:accuracy_parameters_all}, but it still fails to predict the accuracy of the networks with a high number of parameters.

\begin{figure}
  \begin{center}
    \includegraphics{\string~/Dropbox/uniqueness_cnn_figures/accuracy_parameters_all.pdf}
  \end{center}
  \vspace{5mm}
  \caption{Highest achieved accuracy as a function of the total number of parameters in the network. Orange points are from the networks with constant numbers of filters, blue points are from the networks with exponentially increasing number of filters. The orange line is fitted to the orange points, the blue line to the blue points and the green line is fitted to all points.}
  \label{fig:accuracy_parameters_all}
\end{figure}


\begin{figure}
  \begin{center}
    \includegraphics{\string~/Dropbox/uniqueness_cnn_figures/accuracy_parameters_split.pdf}
  \end{center}
  \vspace{5mm}
  \caption{Highest achieved accuracy as a function of the total number of parameters in the network. For each plot, the number of dense neurons is fixed. The top row is from networks with exponentially increasing numbers of filters, and the bottom row is from networks  with constant number of filters. The curves are fitted to all but the rightmost data point in each plot.}
  \label{fig:accuracy_parameters_split}
\end{figure}

